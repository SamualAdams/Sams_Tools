{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sam's Databricks Tooling - Comprehensive Workflow Demo\n\nThis notebook demonstrates the complete workflow using all four tools in the Databricks tooling suite:\n\n1. **`tool__workstation`** - Spark session management\n2. **`tool__dag_chainer`** - DataFrame workflow orchestration\n3. **`tool__table_polisher`** - Data standardization\n4. **`tool__table_indexer`** - Entity indexing with persistence\n\n## Philosophy: Unix-like Tool Composition\n\nThese tools follow the Unix philosophy of building small, focused components that can be composed together to create powerful data processing workflows.",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Quick Spark Test - Run this first to verify setup\n",
    "print(\"üîß Testing Spark Setup...\")\n",
    "\n",
    "import os\n",
    "# Ensure Java 17 is used\n",
    "os.environ['JAVA_HOME'] = '/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home'\n",
    "\n",
    "try:\n",
    "    from tool__workstation import get_spark\n",
    "    spark = get_spark('local_basic')\n",
    "    print(f\"‚úÖ Spark {spark.version} session created successfully!\")\n",
    "    print(f\"‚òï Java Home: {os.environ.get('JAVA_HOME')}\")\n",
    "    \n",
    "    # Simple test\n",
    "    test_df = spark.range(3).toDF(\"number\")\n",
    "    count = test_df.count()\n",
    "    print(f\"üß™ Test DataFrame created with {count} rows\")\n",
    "    \n",
    "    spark.stop()\n",
    "    print(\"üî• Spark session stopped - ready for main demo!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Setup Error: {e}\")\n",
    "    print(\"üí° Make sure Java 17 is installed: brew install openjdk@17\")"
   ],
   "id": "3db301ad8ed34a19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T14:44:58.270209Z",
     "start_time": "2025-09-22T14:44:58.077424Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 1: Import all tools and initialize workstation\n",
    "print(\"üöÄ Initializing Sam's Databricks Tooling Suite\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Import all our tools\n",
    "from tool__workstation import get_spark, is_spark_active, spark_health_check\n",
    "from tool__dag_chainer import DagChain\n",
    "from tool__table_polisher import polish\n",
    "from tool__table_indexer import TableIndexer\n",
    "\n",
    "# Initialize Spark session using workstation\n",
    "spark = get_spark(\"local_delta\")\n",
    "print(f\"‚úÖ Spark session active: {spark.version}\")\n",
    "\n",
    "# Verify health\n",
    "health = spark_health_check()\n",
    "print(f\"üìä Delta Lake enabled: {health['delta_enabled']}\")\n",
    "print(f\"üîß Session ID: {health['session_id']}\")\n",
    "print()"
   ],
   "id": "c595afa8fe168b5c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing Sam's Databricks Tooling Suite\n",
      "==================================================\n",
      "‚úÖ Spark session active: 4.0.1\n",
      "üìä Delta Lake enabled: True\n",
      "üîß Session ID: local-1758552298134\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/09/22 09:44:58 WARN SparkSession: Cannot use io.delta.sql.DeltaSparkSessionExtension to configure session extensions.\n",
      "java.lang.ClassNotFoundException: io.delta.sql.DeltaSparkSessionExtension\n",
      "\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:445)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:592)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:525)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:467)\n",
      "\tat org.apache.spark.util.SparkClassUtils.classForName(SparkClassUtils.scala:41)\n",
      "\tat org.apache.spark.util.SparkClassUtils.classForName$(SparkClassUtils.scala:36)\n",
      "\tat org.apache.spark.util.Utils$.classForName(Utils.scala:99)\n",
      "\tat org.apache.spark.sql.classic.SparkSession$.$anonfun$applyExtensions$2(SparkSession.scala:1056)\n",
      "\tat org.apache.spark.sql.classic.SparkSession$.$anonfun$applyExtensions$2$adapted(SparkSession.scala:1054)\n",
      "\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n",
      "\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:935)\n",
      "\tat org.apache.spark.sql.classic.SparkSession$.org$apache$spark$sql$classic$SparkSession$$applyExtensions(SparkSession.scala:1054)\n",
      "\tat org.apache.spark.sql.classic.SparkSession$.applyAndLoadExtensions(SparkSession.scala:1038)\n",
      "\tat org.apache.spark.sql.classic.SparkSession.<init>(SparkSession.scala:116)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Constructor.java:500)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:481)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 2: Load sample demand data and create workflow chain\n",
    "print(\"üìä Loading Sample Demand Data\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Create sample demand planning data with realistic business scenario\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "from datetime import date\n",
    "\n",
    "# Sample data schema\n",
    "schema = StructType([\n",
    "    StructField(\"Customer Name\", StringType(), True),\n",
    "    StructField(\"Plant_Location\", StringType(), True), \n",
    "    StructField(\"Material-Code\", StringType(), True),\n",
    "    StructField(\"Demand Qty\", IntegerType(), True),\n",
    "    StructField(\"Forecast_Amount\", DoubleType(), True),\n",
    "    StructField(\"Date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Sample data with messy column names and values (realistic scenario)\n",
    "sample_data = [\n",
    "    (\"  ACME Corp  \", \"PLANT_001\", \"MAT-12345\", 1000, 95000.50, date(2024, 1, 15)),\n",
    "    (\"acme corp\", \"plant_001\", \"mat-12345\", 1200, 110000.00, date(2024, 2, 15)),\n",
    "    (\"Global Industries\", \"PLANT_002\", \"MAT-67890\", 800, 75000.25, date(2024, 1, 20)),\n",
    "    (\"  Tech Solutions LLC  \", \"Plant_003\", \"mat-11111\", 1500, 140000.00, date(2024, 1, 25)),\n",
    "    (\"ACME Corp\", \"Plant_001\", \"MAT-22222\", 900, 85000.75, date(2024, 2, 10)),\n",
    "    (None, \"PLANT_002\", \"mat-67890\", 600, 55000.00, date(2024, 2, 20)),\n",
    "    (\"tech solutions llc\", \"PLANT_003\", \"MAT-11111\", 1800, 165000.50, date(2024, 2, 25))\n",
    "]\n",
    "\n",
    "# Create DataFrame with messy data\n",
    "df_raw_demand = spark.createDataFrame(sample_data, schema)\n",
    "\n",
    "# Initialize workflow chain\n",
    "chain = DagChain()\n",
    "chain.dag__raw_import = df_raw_demand\n",
    "\n",
    "print(\"‚úÖ Raw data loaded into chain\")\n",
    "chain.look(0)  # Show raw data"
   ],
   "id": "35043e32a7fe1613"
  },
  {
   "cell_type": "code",
   "id": "f33yxqwsmqj",
   "source": "# Step 3: Apply table polisher for data standardization\nprint(\"üßπ Applying Table Polisher - Data Standardization\")\nprint(\"-\" * 45)\n\n# Apply polish() function to standardize the messy data\nchain.dag__polished_data = polish(chain.dag__raw_import)\n\nprint(\"‚úÖ Data standardized with table polisher\")\nprint(\"üìã Notice the changes:\")\nprint(\"   ‚Ä¢ Column names: lowercase, special chars ‚Üí underscores\")\nprint(\"   ‚Ä¢ Customer values: trimmed, normalized\")\nprint(\"   ‚Ä¢ Consistent ordering: key columns first\")\nprint()\n\n# Show the polished data\nchain.look(-1)  # Look at latest (polished data)\n\nprint(\"\\nüìä Comparison - Before vs After Polishing:\")\nprint(\"   BEFORE: 'Customer Name', 'Plant_Location', 'Material-Code'\")\nprint(\"   AFTER:  'customer_name', 'plant_location', 'material_code'\")\nprint(\"   + Data values cleaned and normalized\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "z5a1m8dfop",
   "source": "# Step 4: Apply TableIndexer for persistent entity indexing\nprint(\"üî¢ Applying Table Indexer - Entity Indexing\")\nprint(\"-\" * 40)\n\n# Create TableIndexer with polished data\nindexer = TableIndexer(chain.dag__polished_data)\n\nprint(\"üìä Indexing entities:\")\nprint(\"   ‚Ä¢ Customers: Persistent mapping to consecutive integers\")\nprint(\"   ‚Ä¢ Plants: Separate index mapping\")  \nprint(\"   ‚Ä¢ Materials: Separate index mapping\")\nprint(\"   ‚Ä¢ Stored in Delta tables for consistency across runs\")\nprint()\n\n# Apply indexing for each entity type\nchain.dag__indexed_customers = indexer.customer(\"customer_name\")\nchain.dag__indexed_plants = indexer.plant(\"plant_location\") \nchain.dag__indexed_materials = indexer.material(\"material_code\")\n\nprint(\"‚úÖ Entity indexing completed\")\nprint(\"üìã New columns added:\")\nprint(\"   ‚Ä¢ Index__customer_name - Customer indices\")\nprint(\"   ‚Ä¢ Index__plant_location - Plant indices\")\nprint(\"   ‚Ä¢ Index__material_code - Material indices\")\nprint()\n\n# Show final indexed data\nchain.look(-1)  # Look at materials (latest)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2phdhgcc54w",
   "source": "# Step 5: Workflow inspection and analysis\nprint(\"üîç Workflow Analysis & Chain Inspection\")\nprint(\"-\" * 38)\n\nprint(\"üìä Complete workflow trace:\")\nchain.trace(shape=True)  # Show all DataFrames with row counts\n\nprint(\"\\nüéØ Business Value Demonstration:\")\nprint(\"   ‚Ä¢ Started with messy, inconsistent data\")\nprint(\"   ‚Ä¢ Applied systematic standardization\")  \nprint(\"   ‚Ä¢ Created persistent entity mappings\")\nprint(\"   ‚Ä¢ Ready for ML models and analytics\")\n\nprint(\"\\nüíæ Persistence Features:\")\nprint(\"   ‚Ä¢ Entity indices persist across Spark sessions\")\nprint(\"   ‚Ä¢ New entities get consecutive indices\")\nprint(\"   ‚Ä¢ Race condition safe with Delta MERGE\")\nprint(\"   ‚Ä¢ Catalog: test_catalog.supply_chain\")\n\nprint(\"\\nüîß Tool Composition Benefits:\")\nprint(\"   ‚Ä¢ Each tool focused on single responsibility\")\nprint(\"   ‚Ä¢ Tools compose naturally in pipelines\")\nprint(\"   ‚Ä¢ Consistent session management via workstation\")\nprint(\"   ‚Ä¢ Visible intermediate steps via dag chainer\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "er2soyk48pd",
   "source": "# Step 6: Demonstrate Delta table persistence\nprint(\"üíæ Delta Table Persistence Demo\")\nprint(\"-\" * 30)\n\nprint(\"üìã Created mapping tables:\")\n\n# Show the mapping tables that were created\nmapping_tables = [\n    \"test_catalog.supply_chain.mapping__active_customers\",\n    \"test_catalog.supply_chain.mapping__active_plants\", \n    \"test_catalog.supply_chain.mapping__active_materials\"\n]\n\nfor table in mapping_tables:\n    try:\n        df_mapping = spark.table(table)\n        count = df_mapping.count()\n        print(f\"   ‚úÖ {table}: {count} entities\")\n        if count > 0:\n            print(f\"      Sample: {df_mapping.take(2)}\")\n    except Exception as e:\n        print(f\"   ‚ùå {table}: Not yet created\")\n\nprint(f\"\\nüîÑ Persistence Test:\")\nprint(\"   ‚Ä¢ Run this notebook again - entities keep same indices\")\nprint(\"   ‚Ä¢ Add new entities - they get next available indices\") \nprint(\"   ‚Ä¢ Multiple concurrent sessions - no conflicts (Delta MERGE)\")\n\nprint(f\"\\nüìà Production Ready Features:\")\nprint(\"   ‚Ä¢ Auto-compaction enabled on mapping tables\")\nprint(\"   ‚Ä¢ Write optimization enabled\")\nprint(\"   ‚Ä¢ Catalog/schema auto-creation\")\nprint(\"   ‚Ä¢ Comprehensive error handling and logging\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "91lg873heom",
   "source": "## Summary: Complete Toolset Integration\n\nThis demo showed the power of **Unix-like tool composition** for demand planning workflows:\n\n### Tools Used:\n1. **Workstation** ‚Üí Centralized Spark session management\n2. **DAG Chainer** ‚Üí Workflow orchestration with visibility\n3. **Table Polisher** ‚Üí Consistent data standardization  \n4. **Table Indexer** ‚Üí Persistent entity indexing\n\n### Key Benefits:\n- **Composable**: Tools work together naturally\n- **Visible**: Every step tracked and inspectable\n- **Persistent**: Entity mappings survive session restarts\n- **Race-safe**: Concurrent access handled properly\n- **Scalable**: Ready for production Databricks environments\n\n### Next Steps:\n- Load your own CSV/Parquet data instead of sample data\n- Extend with additional business logic transformations\n- Write final results to gold layer tables using `chain.write()`\n- Create custom tools following the same patterns\n\n*The toolset is designed to grow with your needs while maintaining simplicity and composability.*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
