{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sam's Databricks Tooling - Comprehensive Workflow Demo\n\nThis notebook demonstrates the complete workflow using all four tools in the Databricks tooling suite:\n\n1. **`tool__workstation`** - Spark session management\n2. **`tool__dag_chainer`** - DataFrame workflow orchestration\n3. **`tool__table_polisher`** - Data standardization\n4. **`tool__table_indexer`** - Entity indexing with persistence\n\n## Philosophy: Unix-like Tool Composition\n\nThese tools follow the Unix philosophy of building small, focused components that can be composed together to create powerful data processing workflows.",
   "id": "8a77807f92f26ee"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Quick Spark Test - Run this first to verify setup\nprint(\"üîß Testing Spark Setup...\")\n\n# Import required modules\nimport os\n\n# Ensure Java 17 is used (for local environments only)\nos.environ['JAVA_HOME'] = '/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home'\n\ntry:\n    # Import our workstation tools\n    from tool__workstation import get_spark, spark_health_check\n    \n    # Test session creation with auto-detection\n    spark = get_spark('auto')  # Auto-detect environment\n    print(f\"‚úÖ Spark {spark.version} session created successfully!\")\n    print(f\"‚òï Java Home: {os.environ.get('JAVA_HOME')}\")\n    \n    # Health check with safe key access\n    health = spark_health_check()\n    print(f\"üìä Session active: {health.get('session_active', False)}\")\n    print(f\"üåç Environment: {health.get('environment', 'unknown')}\")\n    print(f\"üì¶ Delta enabled: {health.get('delta_enabled', False)}\")\n    if health.get('error'):\n        print(f\"‚ö†Ô∏è  Health check warning: {health['error']}\")\n    \n    # Simple test - create a range DataFrame\n    test_df = spark.range(3).toDF(\"number\")\n    count = test_df.count()\n    print(f\"üß™ Test DataFrame created with {count} rows\")\n    \n    # Environment-aware session cleanup\n    if health.get('environment') == 'local':\n        try:\n            spark.stop()\n            print(\"üî• Local Spark session stopped - ready for main demo!\")\n        except:\n            print(\"üî• Session handled by workstation - ready for main demo!\")\n    else:\n        print(\"üî• Databricks session ready - continuing with demo!\")\n        \nexcept Exception as e:\n    print(f\"‚ùå Setup Error: {e}\")\n    print(\"üí° Troubleshooting:\")\n    print(\"   - Local: Make sure Java 17 is installed: brew install openjdk@17\")\n    print(\"   - Databricks: Ensure the notebook is running on an active cluster\")\n    print(\"   - Check that all tool files are uploaded to your Databricks workspace\")",
   "id": "3db301ad8ed34a19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-22T14:44:58.270209Z",
     "start_time": "2025-09-22T14:44:58.077424Z"
    }
   },
   "cell_type": "code",
   "source": "# Step 1: Import all tools and dependencies\nprint(\"üöÄ Initializing Sam's Databricks Tooling Suite\")\nprint(\"=\" * 50)\n\n# Import all required Python libraries\nimport os\nfrom datetime import date\n\n# Import all PySpark components\nfrom pyspark.sql import functions as F\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n\n# Import all our custom tools\nfrom tool__workstation import get_spark, is_spark_active, spark_health_check\nfrom tool__dag_chainer import DagChain\nfrom tool__table_polisher import polish\nfrom tool__table_indexer import TableIndexer\n\n# Initialize Spark session using workstation (auto-detect environment)\nspark = get_spark(\"auto\")  # Auto-detects local vs Databricks\nprint(f\"‚úÖ Spark session active: {spark.version}\")\n\n# Verify health with safe key access\nhealth = spark_health_check()\nprint(f\"üìä Session active: {health.get('session_active', False)}\")\nprint(f\"üåç Environment: {health.get('environment', 'unknown')}\")\nprint(f\"üì¶ Delta enabled: {health.get('delta_enabled', False)}\")\nprint(f\"üîß Session ID: {health.get('session_id', 'unknown')}\")\nif health.get('error'):\n    print(f\"‚ö†Ô∏è  Health check note: {health['error']}\")\nprint(\"üéØ All imports and session ready!\")",
   "id": "c595afa8fe168b5c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Step 2: Load sample demand data and create workflow chain\nprint(\"üìä Loading Sample Demand Data\")\nprint(\"-\" * 30)\n\n# Create sample demand planning data with realistic business scenario\n# Note: All imports are handled in Step 1 cell above\n\n# Sample data schema - defines the structure of our messy input data\nschema = StructType([\n    StructField(\"Customer Name\", StringType(), True),\n    StructField(\"Plant_Location\", StringType(), True), \n    StructField(\"Material-Code\", StringType(), True),\n    StructField(\"Demand Qty\", IntegerType(), True),\n    StructField(\"Forecast_Amount\", DoubleType(), True),\n    StructField(\"Date\", DateType(), True)\n])\n\n# Sample data with messy column names and values (realistic scenario)\nsample_data = [\n    (\"  ACME Corp  \", \"PLANT_001\", \"MAT-12345\", 1000, 95000.50, date(2024, 1, 15)),\n    (\"acme corp\", \"plant_001\", \"mat-12345\", 1200, 110000.00, date(2024, 2, 15)),\n    (\"Global Industries\", \"PLANT_002\", \"MAT-67890\", 800, 75000.25, date(2024, 1, 20)),\n    (\"  Tech Solutions LLC  \", \"Plant_003\", \"mat-11111\", 1500, 140000.00, date(2024, 1, 25)),\n    (\"ACME Corp\", \"Plant_001\", \"MAT-22222\", 900, 85000.75, date(2024, 2, 10)),\n    (None, \"PLANT_002\", \"mat-67890\", 600, 55000.00, date(2024, 2, 20)),\n    (\"tech solutions llc\", \"PLANT_003\", \"MAT-11111\", 1800, 165000.50, date(2024, 2, 25))\n]\n\n# Create DataFrame with messy data using the session from Step 1\ndf_raw_demand = spark.createDataFrame(sample_data, schema)\n\n# Initialize workflow chain\nchain = DagChain()\nchain.dag__raw_import = df_raw_demand\n\nprint(\"‚úÖ Raw data loaded into chain\")\nchain.look(0)  # Show raw data",
   "id": "35043e32a7fe1613"
  },
  {
   "cell_type": "code",
   "id": "f33yxqwsmqj",
   "source": "# Step 3: Apply table polisher for data standardization\nprint(\"üßπ Applying Table Polisher - Data Standardization\")\nprint(\"-\" * 45)\n\n# Apply polish() function to standardize the messy data\nchain.dag__polished_data = polish(chain.dag__raw_import)\n\nprint(\"‚úÖ Data standardized with table polisher\")\nprint(\"üìã Notice the changes:\")\nprint(\"   ‚Ä¢ Column names: lowercase, special chars ‚Üí underscores\")\nprint(\"   ‚Ä¢ Customer values: trimmed, normalized\")\nprint(\"   ‚Ä¢ Consistent ordering: key columns first\")\nprint()\n\n# Show the polished data\nchain.look(-1)  # Look at latest (polished data)\n\nprint(\"\\nüìä Comparison - Before vs After Polishing:\")\nprint(\"   BEFORE: 'Customer Name', 'Plant_Location', 'Material-Code'\")\nprint(\"   AFTER:  'customer_name', 'plant_location', 'material_code'\")\nprint(\"   + Data values cleaned and normalized\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "z5a1m8dfop",
   "source": "# Step 4: Apply TableIndexer for persistent entity indexing\nprint(\"üî¢ Applying Table Indexer - Entity Indexing\")\nprint(\"-\" * 40)\n\n# Create TableIndexer with polished data\nindexer = TableIndexer(chain.dag__polished_data)\n\nprint(\"üìä Indexing entities:\")\nprint(\"   ‚Ä¢ Customers: Persistent mapping to consecutive integers\")\nprint(\"   ‚Ä¢ Plants: Separate index mapping\")  \nprint(\"   ‚Ä¢ Materials: Separate index mapping\")\nprint(\"   ‚Ä¢ Stored in Delta tables for consistency across runs\")\nprint()\n\n# Apply indexing for each entity type\nchain.dag__indexed_customers = indexer.customer(\"customer_name\")\nchain.dag__indexed_plants = indexer.plant(\"plant_location\") \nchain.dag__indexed_materials = indexer.material(\"material_code\")\n\nprint(\"‚úÖ Entity indexing completed\")\nprint(\"üìã New columns added:\")\nprint(\"   ‚Ä¢ Index__customer_name - Customer indices\")\nprint(\"   ‚Ä¢ Index__plant_location - Plant indices\")\nprint(\"   ‚Ä¢ Index__material_code - Material indices\")\nprint()\n\n# Show final indexed data\nchain.look(-1)  # Look at materials (latest)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "2phdhgcc54w",
   "source": "# Step 5: Workflow inspection and analysis\nprint(\"üîç Workflow Analysis & Chain Inspection\")\nprint(\"-\" * 38)\n\nprint(\"üìä Complete workflow trace:\")\nchain.trace(shape=True)  # Show all DataFrames with row counts\n\nprint(\"\\nüéØ Business Value Demonstration:\")\nprint(\"   ‚Ä¢ Started with messy, inconsistent data\")\nprint(\"   ‚Ä¢ Applied systematic standardization\")  \nprint(\"   ‚Ä¢ Created persistent entity mappings\")\nprint(\"   ‚Ä¢ Ready for ML models and analytics\")\n\nprint(\"\\nüíæ Persistence Features:\")\nprint(\"   ‚Ä¢ Entity indices persist across Spark sessions\")\nprint(\"   ‚Ä¢ New entities get consecutive indices\")\nprint(\"   ‚Ä¢ Race condition safe with Delta MERGE\")\nprint(\"   ‚Ä¢ Catalog: test_catalog.supply_chain\")\n\nprint(\"\\nüîß Tool Composition Benefits:\")\nprint(\"   ‚Ä¢ Each tool focused on single responsibility\")\nprint(\"   ‚Ä¢ Tools compose naturally in pipelines\")\nprint(\"   ‚Ä¢ Consistent session management via workstation\")\nprint(\"   ‚Ä¢ Visible intermediate steps via dag chainer\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "er2soyk48pd",
   "source": "# Step 6: Demonstrate Delta table persistence\nprint(\"üíæ Delta Table Persistence Demo\")\nprint(\"-\" * 30)\n\nprint(\"üìã Created mapping tables:\")\n\n# Show the mapping tables that were created\nmapping_tables = [\n    \"test_catalog.supply_chain.mapping__active_customers\",\n    \"test_catalog.supply_chain.mapping__active_plants\", \n    \"test_catalog.supply_chain.mapping__active_materials\"\n]\n\nfor table in mapping_tables:\n    try:\n        df_mapping = spark.table(table)\n        count = df_mapping.count()\n        print(f\"   ‚úÖ {table}: {count} entities\")\n        if count > 0:\n            print(f\"      Sample: {df_mapping.take(2)}\")\n    except Exception as e:\n        print(f\"   ‚ùå {table}: Not yet created\")\n\nprint(f\"\\nüîÑ Persistence Test:\")\nprint(\"   ‚Ä¢ Run this notebook again - entities keep same indices\")\nprint(\"   ‚Ä¢ Add new entities - they get next available indices\") \nprint(\"   ‚Ä¢ Multiple concurrent sessions - no conflicts (Delta MERGE)\")\n\nprint(f\"\\nüìà Production Ready Features:\")\nprint(\"   ‚Ä¢ Auto-compaction enabled on mapping tables\")\nprint(\"   ‚Ä¢ Write optimization enabled\")\nprint(\"   ‚Ä¢ Catalog/schema auto-creation\")\nprint(\"   ‚Ä¢ Comprehensive error handling and logging\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "91lg873heom",
   "source": "## Summary: Complete Toolset Integration\n\nThis demo showed the power of **Unix-like tool composition** for demand planning workflows:\n\n### Tools Used:\n1. **Workstation** ‚Üí Centralized Spark session management\n2. **DAG Chainer** ‚Üí Workflow orchestration with visibility\n3. **Table Polisher** ‚Üí Consistent data standardization  \n4. **Table Indexer** ‚Üí Persistent entity indexing\n\n### Key Benefits:\n- **Composable**: Tools work together naturally\n- **Visible**: Every step tracked and inspectable\n- **Persistent**: Entity mappings survive session restarts\n- **Race-safe**: Concurrent access handled properly\n- **Scalable**: Ready for production Databricks environments\n\n### Next Steps:\n- Load your own CSV/Parquet data instead of sample data\n- Extend with additional business logic transformations\n- Write final results to gold layer tables using `chain.write()`\n- Create custom tools following the same patterns\n\n*The toolset is designed to grow with your needs while maintaining simplicity and composability.*",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}